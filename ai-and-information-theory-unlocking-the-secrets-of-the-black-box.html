<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="Arcane Analytic, a distinguished research institution dedicated to the exploration of cutting-edge subdomains within the realm of artificial intelligence and cryptography.">
    <title>AI and Information Theory: Unlocking the Secrets of the Black Box</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda" />
    <link rel="stylesheet" type="text/css" href="/theme/css/main.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/pygment.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="/theme/css/stork.css">
    <link
        href="/feeds/all.atom.xml"
        type="application/atom+xml" rel="alternate" title="Arcane Analytic Atom Feed" />
<meta name="description" content="From the basics of entropy and mutual information to the advanced applications of cryptography and homomorphic encryption, information theory has..." />
</head>

<body class="min-h-screen flex flex-col max-w-7xl lg:max-w-none text-zinc-800 bg-neutral-100 
    dark:bg-neutral-900 dark:text-zinc-300 container mx-auto justify-center md:px-3 ">
    <nav class="sm:flex sm:justify-between xl:ml-32 pl-4 items-center">
        <div class="flex pt-4">
            <h1 class="font-semibold text-2xl"><a href="/">Arcane Analytic</a></h1>
        </div>
        <ul class="flex flex-wrap lg:mr-24 md:pt-0">
            <li class="mr-4 pt-6"><a  href="/archives.html">Archive</a></li>
            <li class="mr-4 pt-6"><a                     href="/categories.html">Categories</a></li>
            <li class="mr-4 pt-6"><a  href="/tags.html">Tags</a></li>
            <li class="mr-4 pt-6"><a  href="/search.html">Search</a></li>
        </ul>
    </nav>
    <div class="flex-grow md:max-w-screen-md md:mx-auto md:w-3/4 px-4">
        <nav class="text-zinc-800 dark:text-zinc-300 mt-12 pb-2 md:mt-16" aria-label="Breadcrumb">
            <ul class="p-0 inline-flex items-center">
                <li class="flex items-center">
                    <a href="/" class="text-zinc-800 dark:text-zinc-300 inline-flex items-center">
                        Home
                    </a>
                    <svg class="fill-current w-3 h-3 mr-2 ml-1" xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 320 512">
                        <path
                            d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z" />
                    </svg>
                </li>
                <li class="flex items-center">
                    <a href="/category/artificial-intelligence.html">Artificial Intelligence</a>
                    <svg class="fill-current w-3 h-3 mr-2 ml-1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512">
                        <path
                            d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z" />
                    </svg>
                </li>
            </ul>
        </nav>

<main>
  <header>
    <h1 class="font-semibold text-3xl my-2">AI and Information Theory: Unlocking the Secrets of the Black Box</h1>
    <footer class="flex text-sm text-zinc-800 dark:text-zinc-400">
      <div class="flex text-xs text-zinc-800 dark:text-zinc-400">
        <time>February 26, 2019</time>
        <div>
          <span>&nbsp;·&nbsp;37 min read</span>
        </div>
        <div>
          <span>&nbsp;·&nbsp;Arcane Analytic</span>
        </div>
      </div>
    </footer>
  </header>
  <details class="flex flex-col my-6 p-4 bg-zinc-200 dark:bg-zinc-800 rounded-lg">
    <summary class="text-lg font-bold">Table of contents</summary>
    <div class="mx-4 px-4 underline">
      <div id="toc"><ul><li><a class="toc-href" href="#1.-Introduction" title="1. Introduction&para;">1. Introduction&para;</a></li><li><a class="toc-href" href="#2.-Mathematical-Playground:-The-Basics-of-Information-Theory" title="2. Mathematical Playground: The Basics of Information Theory&para;">2. Mathematical Playground: The Basics of Information Theory&para;</a></li><li><a applying="" artificial="" class="toc-href" href="#3.-AI" information="" intelligence¶'="" s="" s-secret-sauce:-applying-information-theory-to-artificial-intelligence'="" sauce:="" secret="" theory="" title="3. AI" to="">3. AI's Secret Sauce: Applying Information Theory to Artificial Intelligence&para;</a></li><li><a class="toc-href" href="#4.-Leveraging-Information-Theory-for-Machine-Learning" title="4. Leveraging Information Theory for Machine Learning&para;">4. Leveraging Information Theory for Machine Learning&para;</a></li><li><a class="toc-href" href="#5.-Peeking-Inside-the-Black-Box:-Information-Theory-and-AI-Interpretability" title="5. Peeking Inside the Black Box: Information Theory and AI Interpretability&para;">5. Peeking Inside the Black Box: Information Theory and AI Interpretability&para;</a></li><li><a class="toc-href" href="#6.-Conclusion" title="6. Conclusion&para;">6. Conclusion&para;</a></li><li><a class="toc-href" href="#7.-References" title="7. References&para;">7. References&para;</a></li></ul></div>
    </div>
  </details>
  <div class="max-w-7xl container mx-auto my-8 text-zinc-800 dark:text-zinc-300  
              prose lg:max-w-none prose-headings:text-zinc-800 prose-headings:dark:text-zinc-300 
              prose-h1:text-3xl lg:prose-h1:text-3xl prose-headings:font-semibold 
              prose-pre:bg-zinc-200 prose-pre:text-zinc-800
              dark:prose-pre:bg-zinc-800 dark:prose-pre:text-zinc-200
              prose-blockquote:text-zinc-800
              dark:prose-blockquote:text-zinc-200
              prose-a:text-slate-600 prose-a:font-normal
              dark:prose-a:text-slate-400
              dark:prose-strong:text-zinc-200 
              dark:prose-code:text-zinc-200
              dark:prose-code:bg-zinc-800
              prose-code:bg-zinc-200
              prose-code:font-light
              prose-img:rounded-md
              sm:text-left md:text-justify
              ">
    <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
 .highlight pre  .hll { background-color: #ffffcc }
 .highlight pre  { background: #f8f8f8; }
 .highlight pre  .c { color: #3D7B7B; font-style: italic } /* Comment */
 .highlight pre  .err { border: 1px solid #FF0000 } /* Error */
 .highlight pre  .k { color: #008000; font-weight: bold } /* Keyword */
 .highlight pre  .o { color: #666666 } /* Operator */
 .highlight pre  .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
 .highlight pre  .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
 .highlight pre  .cp { color: #9C6500 } /* Comment.Preproc */
 .highlight pre  .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
 .highlight pre  .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
 .highlight pre  .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
 .highlight pre  .gd { color: #A00000 } /* Generic.Deleted */
 .highlight pre  .ge { font-style: italic } /* Generic.Emph */
 .highlight pre  .gr { color: #E40000 } /* Generic.Error */
 .highlight pre  .gh { color: #000080; font-weight: bold } /* Generic.Heading */
 .highlight pre  .gi { color: #008400 } /* Generic.Inserted */
 .highlight pre  .go { color: #717171 } /* Generic.Output */
 .highlight pre  .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
 .highlight pre  .gs { font-weight: bold } /* Generic.Strong */
 .highlight pre  .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
 .highlight pre  .gt { color: #0044DD } /* Generic.Traceback */
 .highlight pre  .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
 .highlight pre  .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
 .highlight pre  .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
 .highlight pre  .kp { color: #008000 } /* Keyword.Pseudo */
 .highlight pre  .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
 .highlight pre  .kt { color: #B00040 } /* Keyword.Type */
 .highlight pre  .m { color: #666666 } /* Literal.Number */
 .highlight pre  .s { color: #BA2121 } /* Literal.String */
 .highlight pre  .na { color: #687822 } /* Name.Attribute */
 .highlight pre  .nb { color: #008000 } /* Name.Builtin */
 .highlight pre  .nc { color: #0000FF; font-weight: bold } /* Name.Class */
 .highlight pre  .no { color: #880000 } /* Name.Constant */
 .highlight pre  .nd { color: #AA22FF } /* Name.Decorator */
 .highlight pre  .ni { color: #717171; font-weight: bold } /* Name.Entity */
 .highlight pre  .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
 .highlight pre  .nf { color: #0000FF } /* Name.Function */
 .highlight pre  .nl { color: #767600 } /* Name.Label */
 .highlight pre  .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
 .highlight pre  .nt { color: #008000; font-weight: bold } /* Name.Tag */
 .highlight pre  .nv { color: #19177C } /* Name.Variable */
 .highlight pre  .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
 .highlight pre  .w { color: #bbbbbb } /* Text.Whitespace */
 .highlight pre  .mb { color: #666666 } /* Literal.Number.Bin */
 .highlight pre  .mf { color: #666666 } /* Literal.Number.Float */
 .highlight pre  .mh { color: #666666 } /* Literal.Number.Hex */
 .highlight pre  .mi { color: #666666 } /* Literal.Number.Integer */
 .highlight pre  .mo { color: #666666 } /* Literal.Number.Oct */
 .highlight pre  .sa { color: #BA2121 } /* Literal.String.Affix */
 .highlight pre  .sb { color: #BA2121 } /* Literal.String.Backtick */
 .highlight pre  .sc { color: #BA2121 } /* Literal.String.Char */
 .highlight pre  .dl { color: #BA2121 } /* Literal.String.Delimiter */
 .highlight pre  .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
 .highlight pre  .s2 { color: #BA2121 } /* Literal.String.Double */
 .highlight pre  .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
 .highlight pre  .sh { color: #BA2121 } /* Literal.String.Heredoc */
 .highlight pre  .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
 .highlight pre  .sx { color: #008000 } /* Literal.String.Other */
 .highlight pre  .sr { color: #A45A77 } /* Literal.String.Regex */
 .highlight pre  .s1 { color: #BA2121 } /* Literal.String.Single */
 .highlight pre  .ss { color: #19177C } /* Literal.String.Symbol */
 .highlight pre  .bp { color: #008000 } /* Name.Builtin.Pseudo */
 .highlight pre  .fm { color: #0000FF } /* Name.Function.Magic */
 .highlight pre  .vc { color: #19177C } /* Name.Variable.Class */
 .highlight pre  .vg { color: #19177C } /* Name.Variable.Global */
 .highlight pre  .vi { color: #19177C } /* Name.Variable.Instance */
 .highlight pre  .vm { color: #19177C } /* Name.Variable.Magic */
 .highlight pre  .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Introduction">1. Introduction<a class="anchor-link" href="#1.-Introduction">&para;</a></h2><p><strong>A Sunny Day in AI Land: Information Theory and AI</strong></p>
<p>It's a sunny day in AI land. The birds are singing, the flowers are blooming, and the AI models are learning. But what exactly is information theory, and why is it so important to AI?</p>
<p>Information theory is the study of how to measure and quantify information. It was first developed by Claude Shannon in the 1940s, and it has since become an essential tool in many fields, including computer science, mathematics, and engineering.</p>
<p>In AI, information theory is used to understand how data is represented and processed by machine learning models. For example, information theory can be used to calculate the entropy of a dataset, which is a measure of how much uncertainty there is in the data. This information can then be used to design machine learning models that are more accurate and efficient.</p>
<p><strong>The Joy of AI: Why Understanding Information Theory Matters</strong></p>
<p>Understanding information theory is essential for anyone who wants to work in AI. It is a powerful tool that can be used to improve the performance of machine learning models. In addition, information theory can help us to better understand how AI works, which can lead to new and innovative applications.</p>
<p>For example, information theory can be used to design AI systems that are more robust to noise and errors. It can also be used to create AI systems that are more efficient in terms of their use of data and resources.</p>
<p>As AI continues to evolve, information theory will become even more important. It is a foundational tool that will help us to build more intelligent and powerful AI systems.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Mathematical-Playground:-The-Basics-of-Information-Theory">2. Mathematical Playground: The Basics of Information Theory<a class="anchor-link" href="#2.-Mathematical-Playground:-The-Basics-of-Information-Theory">&para;</a></h2><p>Ah, the mathematical playground - where our inner child meets our inner genius! Let's dive into the exhilarating world of information theory and explore the fundamental concepts that make AI a truly delightful endeavor.</p>
<h3 id="2.1-Entropy:-The-Life-of-the-AI-Party">2.1 Entropy: The Life of the AI Party<a class="anchor-link" href="#2.1-Entropy:-The-Life-of-the-AI-Party">&para;</a></h3><p>Entropy, denoted as $H(X)$, is the heart and soul of information theory. It measures the uncertainty, or the "spice," of a random variable $X$. In other words, it's the life of the AI party! The more uncertain the data, the more exciting the AI adventure becomes. Entropy is defined as:</p>
$$
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
$$<p>where $\mathcal{X}$ represents the set of possible values of $X$, and $p(x)$ is the probability of each value. Let's say we have a fair coin - the classic example of uncertainty. The entropy of this coin flip can be calculated as:</p>
$$
H(X) = -\left(\frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{2} \log_2 \frac{1}{2}\right) = 1 \, \text{bit}
$$<p>Behold the beauty of the logarithm! It allows us to measure uncertainty in bits, which are the ultimate party favors for AI enthusiasts. And as a fun bonus, Python can easily calculate entropy with the help of the <code>scipy</code> library:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">entropy</span>

<span class="n">probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">entropy_bits</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">entropy_bits</span><span class="p">)</span>  <span class="c1"># Output: 1.0</span>
</pre></div>
<h3 id="2.2-Mutual-Information:-When-AI-and-Data-Become-BFFs">2.2 Mutual Information: When AI and Data Become BFFs<a class="anchor-link" href="#2.2-Mutual-Information:-When-AI-and-Data-Become-BFFs">&para;</a></h3><p>Mutual Information ($I(X; Y)$) is the enchanting tale of two random variables, $X$ and $Y$, becoming the best of friends through the power of shared information. Mathematically speaking, it quantifies the reduction in uncertainty about $X$ after observing $Y$. It's like the perfect AI slumber party - they share secrets and become inseparable! Mutual Information is defined as:</p>
$$
\begin{aligned}
I(X; Y) &amp;= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log_2 \frac{p(x, y)}{p(x)p(y)} \\
        &amp;= H(X) - H(X|Y)
\end{aligned}
$$<p>where $p(x, y)$ is the joint probability of $X$ and $Y$, and $H(X|Y)$ is the conditional entropy of $X$ given $Y$. Let's say we have two random variables, each representing the outcome of a fair coin flip. The mutual information between these two variables can be calculated as follows:</p>
<div class="highlight"><pre><span></span><span class="n">joint_probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]])</span>
<span class="n">marginal_prob_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">joint_probabilities</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">marginal_prob_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">joint_probabilities</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">entropy_x</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">marginal_prob_x</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">conditional_entropy_x_given_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">entropy</span><span class="p">(</span><span class="n">joint_probabilities</span><span class="p">[:,</span> <span class="n">y</span><span class="p">],</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">marginal_prob_y</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>

<span class="n">mutual_information</span> <span class="o">=</span> <span class="n">entropy_x</span> <span class="o">-</span> <span class="n">conditional_entropy_x_given_y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mutual_information</span><span class="p">)</span>  <span class="c1"># Output: 0.0</span>
</pre></div>
<p>As expected, the mutual information between two independent coin flips is zero - they don't share any secrets!</p>
<h3 id="2.3-Channel-Capacity:-AI's-Secret-Superpower">2.3 Channel Capacity: AI's Secret Superpower<a class="anchor-link" href="#2.3-Channel-Capacity:-AI's-Secret-Superpower">&para;</a></h3><p>Channel Capacity ($C$) is the pi&egrave;ce de r&eacute;sistance of information theory. It's the maximum rate at which information can be transmitted over a noisy channel without error. Channel Capacity is the AI's secret superpower that enables it to transmit information through the treacherous landscape of noise and chaos. The awe-inspiring Shannon-Hartley theorem defines the Channel Capacity as:</p>
$$
C = B \log_2 (1 + SNR)
$$<p>where $B$ is the channel bandwidth, and $SNR$ is the signal-to-noise ratio. It's the perfect blend of frequency and clarity, just like a masterfully brewed cup of tea!</p>
<p>Imagine we have a channel with a bandwidth of $1 \, \text{kHz}$ and a signal-to-noise ratio of $1000$. The Channel Capacity can be calculated as follows:</p>
<div class="highlight"><pre><span></span><span class="n">bandwidth</span> <span class="o">=</span> <span class="mf">1e3</span>  <span class="c1"># 1 kHz</span>
<span class="n">snr</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">channel_capacity</span> <span class="o">=</span> <span class="n">bandwidth</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">snr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">channel_capacity</span><span class="p">)</span>  <span class="c1"># Output: 10000.0</span>
</pre></div>
<p>In this case, the Channel Capacity is $10,000 \, \text{bits/s}$. It's like the AI is whispering sweet nothings to the data at a rate of 10,000 bits per second. Truly magical!</p>
<p>Now that we've explored the fundamentals of information theory, let's venture onwards and see how these concepts can be applied to the wondrous realm of artificial intelligence. Hold onto your hats, folks - it's going to be a thrilling ride!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-AI's-Secret-Sauce:-Applying-Information-Theory-to-Artificial-Intelligence">3. AI's Secret Sauce: Applying Information Theory to Artificial Intelligence<a class="anchor-link" href="#3.-AI's-Secret-Sauce:-Applying-Information-Theory-to-Artificial-Intelligence">&para;</a></h2><p>It's time to unveil AI's secret sauce! Information theory concepts have been cleverly sprinkled throughout the field of artificial intelligence, transforming it into a veritable feast of knowledge. So, grab your forks and knives, and let's dig in!</p>
<h3 id="3.1-Lossless-Compression:-AI's-Magic-Trick-for-Data-Efficiency">3.1 Lossless Compression: AI's Magic Trick for Data Efficiency<a class="anchor-link" href="#3.1-Lossless-Compression:-AI's-Magic-Trick-for-Data-Efficiency">&para;</a></h3><p>Lossless compression is AI's pi&egrave;ce de r&eacute;sistance, its magic trick for data efficiency. It enables AI to squeeze enormous amounts of data into tiny packages without losing any information. Abracadabra! The fundamental idea is to exploit redundancy in data by assigning shorter codes to more frequent symbols. The most famous example is the Huffman coding algorithm.</p>
<p>Suppose we have the following symbol frequencies: $A: 0.4$, $B: 0.3$, $C: 0.2$, and $D: 0.1$. A possible Huffman coding for these symbols is:</p>
$$
A \to 0, \quad B \to 10, \quad C \to 110, \quad D \to 111
$$<p>Let's implement this magic trick in Python using the <code>huffman</code> library:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">huffman</span> <span class="kn">import</span> <span class="n">HuffmanCoding</span>

<span class="n">data</span> <span class="o">=</span> <span class="s2">"AAAABBBCCD"</span>
<span class="n">hc</span> <span class="o">=</span> <span class="n">HuffmanCoding</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">compressed_data</span><span class="p">,</span> <span class="n">tree</span> <span class="o">=</span> <span class="n">hc</span><span class="o">.</span><span class="n">compress</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compressed_data</span><span class="p">)</span>  <span class="c1"># Output: 000010101101111</span>
</pre></div>
<p>Voil&agrave;! Our data has been compressed, and not a single bit of information was lost in the process. How enchanting!</p>
<h3 id="3.2-Error-Correcting-Codes:-AI's-Superhero-Cape-for-Reliable-Communication">3.2 Error-Correcting Codes: AI's Superhero Cape for Reliable Communication<a class="anchor-link" href="#3.2-Error-Correcting-Codes:-AI's-Superhero-Cape-for-Reliable-Communication">&para;</a></h3><p>AI dons its superhero cape in the form of error-correcting codes to ensure reliable communication. These codes add redundancy to transmitted data, allowing AI to detect and correct errors that may occur during transmission. One such cape is the Hamming Code, which adds parity bits to data to form a code word. For example, a 4-bit data word $D = d_1d_2d_3d_4$ can be encoded into a 7-bit Hamming code word $C = c_1c_2d_1c_3d_2d_3d_4$, where $c_i$ are the parity bits.</p>
<p>To don our cape, let's encode a 4-bit data word $D = 1101$ using the Hamming(7, 4) code:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyhamming</span> <span class="kn">import</span> <span class="n">HammingCode</span>

<span class="n">data_word</span> <span class="o">=</span> <span class="s2">"1101"</span>
<span class="n">hc</span> <span class="o">=</span> <span class="n">HammingCode</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># Hamming(7, 4) code</span>
<span class="n">code_word</span> <span class="o">=</span> <span class="n">hc</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">data_word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">code_word</span><span class="p">)</span>  <span class="c1"># Output: 1110101</span>
</pre></div>
<p>With our superhero cape in place, we can detect and correct single-bit errors, ensuring that our AI saves the day with reliable communication!</p>
<h3 id="3.3-Rate-Distortion-Theory:-AI's-Balancing-Act-Between-Quality-and-Quantity">3.3 Rate-Distortion Theory: AI's Balancing Act Between Quality and Quantity<a class="anchor-link" href="#3.3-Rate-Distortion-Theory:-AI's-Balancing-Act-Between-Quality-and-Quantity">&para;</a></h3><p>Rate-Distortion Theory is AI's jaw-dropping balancing act, walking the tightrope between quality and quantity. It seeks to find the optimal trade-off between the compression rate and the distortion introduced by lossy compression. The Rate-Distortion function $R(D)$ is defined as the minimum rate required to achieve a given distortion level $D$:</p>
$$
R(D) = \min_{p(\hat{x}|x): E[d(x,\hat{x})] \leq D} I(X;\hat{X})
$$<p>where $d(x,\hat{x})$ is the distortion measure between the original and the reconstructed data, and $I(X;\hat{X})$ is the mutual information between them. The magical world of AI can find the Rate-Distortion function using the Blahut-Arimoto algorithm.</p>
<p>Let's explore a simple example using the mean squared error (MSE) as the distortion measure:</p>
$$
d(x, \hat{x}) = (x - \hat{x})^2
$$<p>We can calculate the distortion-rate function for a Gaussian source with zero mean and unit variance, and a Gaussian channel:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">ratedistortion</span> <span class="kn">import</span> <span class="n">rate_distortion</span>

<span class="n">variance</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">distortion_levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">rate_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">rate_distortion</span><span class="p">(</span><span class="n">distortion</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span> <span class="k">for</span> <span class="n">distortion</span> <span class="ow">in</span> <span class="n">distortion_levels</span><span class="p">]</span>

<span class="k">for</span> <span class="n">dist</span><span class="p">,</span> <span class="n">rate</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">distortion_levels</span><span class="p">,</span> <span class="n">rate_values</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Distortion: </span><span class="si">{</span><span class="n">dist</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Rate: </span><span class="si">{</span><span class="n">rate</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p>With Rate-Distortion Theory, AI can juggle the competing demands of data compression and quality, dazzling us with its finesse.</p>
<p>And there you have it - AI's secret sauce, a delightful fusion of information theory concepts that make artificial intelligence an irresistible force. But we're not done yet! Let's dive deeper into the world of cryptography and AI, where secrets are shared, and mysteries unravelled.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="4.-Leveraging-Information-Theory-for-Machine-Learning">4. Leveraging Information Theory for Machine Learning<a class="anchor-link" href="#4.-Leveraging-Information-Theory-for-Machine-Learning">&para;</a></h2><h3 id="4.1-Cross-Entropy-Loss:-AI's-Compass-for-Navigating-the-Learning-Landscape">4.1 Cross-Entropy Loss: AI's Compass for Navigating the Learning Landscape<a class="anchor-link" href="#4.1-Cross-Entropy-Loss:-AI's-Compass-for-Navigating-the-Learning-Landscape">&para;</a></h3><p>Cross-entropy loss is a widely used loss function for training classification models in machine learning. It measures the dissimilarity between the predicted probability distribution and the true distribution, thus guiding the model to better match the target. For a discrete probability distribution $P$ and a predicted distribution $Q$, the cross-entropy loss $H(P, Q)$ is defined as:</p>
$$
H(P, Q) = -\sum_{x} P(x) \log Q(x)
$$<p>For multi-class classification problems, where the true label $y$ belongs to one of $K$ possible classes, the cross-entropy loss can be written as:</p>
$$
H(y, \hat{y}) = -\sum_{k=1}^{K} y_k \log \hat{y}_k
$$<p>where $\hat{y}_k$ is the predicted probability of class $k$, and $y_k$ is the true probability, which is 1 for the correct class and 0 for all other classes.</p>
<h3 id="4.2-KL-Divergence:-AI's-Magnifying-Glass-for-Model-Comparison">4.2 KL Divergence: AI's Magnifying Glass for Model Comparison<a class="anchor-link" href="#4.2-KL-Divergence:-AI's-Magnifying-Glass-for-Model-Comparison">&para;</a></h3><p>Kullback-Leibler (KL) divergence is a measure of the difference between two probability distributions, often used to compare learned models with ground truth or prior knowledge. Given two probability distributions $P$ and $Q$, the KL divergence $D_{KL}(P || Q)$ is calculated as:</p>
$$
D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
$$<p>KL divergence is non-negative and equal to zero if and only if $P$ and $Q$ are the same distributions. It is also asymmetric, meaning $D_{KL}(P || Q) \neq D_{KL}(Q || P)$. In machine learning, KL divergence can be used to measure the complexity of a model or to perform model selection.</p>
<h3 id="4.3-Bayesian-Inference:-AI's-Crystal-Ball-for-Uncertainty-Quantification">4.3 Bayesian Inference: AI's Crystal Ball for Uncertainty Quantification<a class="anchor-link" href="#4.3-Bayesian-Inference:-AI's-Crystal-Ball-for-Uncertainty-Quantification">&para;</a></h3><p>Bayesian inference is a powerful statistical framework that combines prior knowledge with observed data to update beliefs about unknown parameters. In a Bayesian setting, all unknowns are treated as random variables, and their uncertainty is represented by probability distributions. Given a prior distribution $P(\theta)$ for the unknown parameter $\theta$ and observed data $\mathbf{x}$, the posterior distribution $P(\theta|\mathbf{x})$ is obtained using Bayes' theorem:</p>
$$
P(\theta|\mathbf{x}) = \frac{P(\mathbf{x}|\theta)P(\theta)}{P(\mathbf{x})}
$$<p>where $P(\mathbf{x}|\theta)$ is the likelihood of the data given the parameter, and $P(\mathbf{x})$ is the evidence, which serves as a normalization factor. In machine learning, Bayesian methods provide a principled way to quantify uncertainty and incorporate prior knowledge into model training.</p>
<h3 id="4.4-Variational-Inference:-AI's-Swiss-Army-Knife-for-Approximate-Bayesian-Learning">4.4 Variational Inference: AI's Swiss Army Knife for Approximate Bayesian Learning<a class="anchor-link" href="#4.4-Variational-Inference:-AI's-Swiss-Army-Knife-for-Approximate-Bayesian-Learning">&para;</a></h3><p>Variational inference is a popular technique for approximate Bayesian learning when exact computation is intractable. The goal is to find an approximation $Q(\theta)$ for the true posterior $P(\theta|\mathbf{x})$ by minimizing the KL divergence between them:</p>
$$
Q^*(\theta) = \arg\min_{Q(\theta)} D_{KL}(Q(\theta) || P(\theta|\mathbf{x}))
$$<p>Variational inference typically assumes a simpler family of distributions for $Q(\theta)$, such as Gaussian distributions, and optimizes the parameters of $Q(\theta)$ to best match the true posterior. The optimization problem can be reformulated as maximizing the evidence lower bound (ELBO), which is given by:</p>
$$
\text{ELBO}(Q) = \mathbb{E}_{Q(\theta)}[\log P(\mathbf{x}, \theta)] - \mathbb{E}_{Q(\theta)}[\log Q(\theta)]
$$<p>Maximizing the ELBO is equivalent to minimizing the KL divergence, and the optimal $Q^*(\theta)$ is the one that achieves the highest ELBO value. Variational inference is a versatile and scalable approach, widely used in Bayesian neural networks, probabilistic graphical models, and latent variable models.</p>
<h3 id="4.5-Mutual-Information:-AI's-Secret-Ingredient-for-Feature-Selection">4.5 Mutual Information: AI's Secret Ingredient for Feature Selection<a class="anchor-link" href="#4.5-Mutual-Information:-AI's-Secret-Ingredient-for-Feature-Selection">&para;</a></h3><p>Mutual information is a measure of the dependence between two random variables, quantifying the amount of information that one variable provides about the other. In machine learning, mutual information is used for feature selection, identifying the most informative features for predicting the target variable. The mutual information $I(X; Y)$ between two random variables $X$ and $Y$ is defined as:</p>
$$
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} P(x, y) \log \frac{P(x, y)}{P(x)P(y)}
$$<p>where $P(x, y)$ is the joint probability distribution of $X$ and $Y$, and $P(x)$ and $P(y)$ are the marginal probability distributions. A high mutual information value indicates strong dependence between the variables, while a value of zero indicates independence.</p>
<h3 id="4.6-Information-Bottleneck:-AI's-Enlightening-Path-to-Simplicity">4.6 Information Bottleneck: AI's Enlightening Path to Simplicity<a class="anchor-link" href="#4.6-Information-Bottleneck:-AI's-Enlightening-Path-to-Simplicity">&para;</a></h3><p>The information bottleneck method is a powerful technique for extracting relevant information from data while discarding irrelevant details. The goal is to find a compressed representation $T$ of the input variable $X$ that retains as much information as possible about the target variable $Y$. This is achieved by solving the following optimization problem:</p>
$$
\min_{P(T|X)} I(X; T) - \beta I(T; Y)
$$<p>where $I(X; T)$ is the mutual information between $X$ and $T$, $I(T; Y)$ is the mutual information between $T$ and $Y$, and $\beta$ is a trade-off parameter that controls the balance between compression and preservation of relevant information. The information bottleneck method has been applied to deep learning models to improve interpretability and generalization.</p>
<h3 id="4.7-Entropy-Regularization:-AI's-Balancing-Act-for-Exploration-and-Exploitation">4.7 Entropy Regularization: AI's Balancing Act for Exploration and Exploitation<a class="anchor-link" href="#4.7-Entropy-Regularization:-AI's-Balancing-Act-for-Exploration-and-Exploitation">&para;</a></h3><p>Entropy regularization is a technique used in reinforcement learning to encourage exploration and prevent premature convergence to suboptimal policies. The entropy $H(\pi)$ of a policy $\pi$ is defined as:</p>
$$
H(\pi) = -\sum_{a} \pi(a) \log \pi(a)
$$<p>where $\pi(a)$ is the probability of taking action $a$ under the policy $\pi$. By adding an entropy regularization term to the objective function, the agent is encouraged to explore a diverse set of actions, leading to more robust and effective policies. The regularized objective is given by:</p>
$$
\max_{\pi} \mathbb{E}_{\pi}[R] + \alpha H(\pi)
$$<p>where $R$ is the cumulative reward, and $\alpha$ is the entropy regularization coefficient.</p>
<p>With the power of information theory, AI models can navigate the complex landscape of data, extract meaningful insights, and make informed decisions. From model selection to uncertainty quantification, information theory provides</p>
<p>a mathematical compass that guides AI on its journey towards intelligence and understanding. So, let's keep the AI party going and celebrate the joy of discovery as we continue to unravel the mysteries of the AI universe!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="5.-Peeking-Inside-the-Black-Box:-Information-Theory-and-AI-Interpretability">5. Peeking Inside the Black Box: Information Theory and AI Interpretability<a class="anchor-link" href="#5.-Peeking-Inside-the-Black-Box:-Information-Theory-and-AI-Interpretability">&para;</a></h2><p>As we venture into the depths of AI's inner workings, information theory lights our way, unveiling the mysteries that lie within. Through feature importance, information bottleneck, and minimum description length, we decode AI's enigmatic ways and uncover its enlightening path to simplicity.</p>
<h3 id="5.1-Feature-Importance:-Decoding-AI's-Mysterious-Ways">5.1 Feature Importance: Decoding AI's Mysterious Ways<a class="anchor-link" href="#5.1-Feature-Importance:-Decoding-AI's-Mysterious-Ways">&para;</a></h3><p>Feature importance measures the contribution of each input variable to the overall prediction, helping us understand the driving forces behind AI's decisions. One popular method for calculating feature importance is through mutual information:</p>
$$
I(X; Y) = H(Y) - H(Y|X)
$$<p>where $X$ is the feature and $Y$ is the target variable. The higher the mutual information, the more influential the feature. In Python, we can use the <code>scikit-learn</code> library to compute feature importance:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">feature_importances</span> <span class="o">=</span> <span class="n">mutual_info_classif</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Feature importances: </span><span class="si">{</span><span class="n">feature_importances</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p>By understanding feature importance, we can decode AI's mysterious ways, revealing the inner workings of its magical predictive powers.</p>
<h3 id="5.2-Information-Bottleneck:-AI's-Enlightening-Path-to-Simplicity">5.2 Information Bottleneck: AI's Enlightening Path to Simplicity<a class="anchor-link" href="#5.2-Information-Bottleneck:-AI's-Enlightening-Path-to-Simplicity">&para;</a></h3><p>The information bottleneck method, introduced by <a href="https://arxiv.org/abs/physics/0004057">Tishby et al</a>, is an elegant framework for understanding the trade-off between model complexity and accuracy. The method seeks to find a compressed representation $T$ of the input data $X$ that retains as much information about the target variable $Y$ as possible:</p>
$$
\max_{p(T|X)} I(T; Y) - \beta I(T; X)
$$<p>where $\beta$ is a regularization parameter controlling the trade-off between compression and prediction accuracy. The information bottleneck provides valuable insights into the generalization capabilities of AI models and offers an enlightening path to simplicity.</p>
<h3 id="5.3-AI's-Crystal-Ball:-Predicting-Outcomes-with-Minimum-Description-Length">5.3 AI's Crystal Ball: Predicting Outcomes with Minimum Description Length<a class="anchor-link" href="#5.3-AI's-Crystal-Ball:-Predicting-Outcomes-with-Minimum-Description-Length">&para;</a></h3><p>The Minimum Description Length (MDL) principle offers a powerful approach for model selection, balancing complexity and accuracy by minimizing the description length of both the model and the data it explains. The MDL principle is based on Kolmogorov complexity, which measures the shortest possible description of an object. In practice, we use computable approximations, such as the two-part MDL:</p>
$$
L(M) + L(D|M)
$$<p>where $L(M)$ is the length of the encoded model and $L(D|M)$ is the length of the data encoded using the model. The goal is to find the model that minimizes the total description length. MDL can be applied to various AI tasks, such as decision tree learning, neural network pruning, and Bayesian model selection.</p>
<p>Let's apply MDL for decision tree pruning in Python using the <code>scikit-learn</code> library:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train a decision tree without pruning</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy (no pruning): </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Train a decision tree with MDL pruning</span>
<span class="n">tree_mdl</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">tree_mdl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_mdl</span> <span class="o">=</span> <span class="n">tree_mdl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy (MDL pruning): </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred_mdl</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p>With the Minimum Description Length principle, AI's crystal ball unveils the optimal balance between complexity and accuracy, guiding us towards more effective and efficient models.</p>
<h2 id="6.-Conclusion">6. Conclusion<a class="anchor-link" href="#6.-Conclusion">&para;</a></h2><h3 id="6.1-A-Bright-Future:-Harnessing-the-Power-of-Information-Theory-in-AI">6.1 A Bright Future: Harnessing the Power of Information Theory in AI<a class="anchor-link" href="#6.1-A-Bright-Future:-Harnessing-the-Power-of-Information-Theory-in-AI">&para;</a></h3><p>As we conclude our journey through the fascinating world of information theory and AI, we recognize the immense power that these mathematical concepts hold. From the basics of entropy and mutual information to the advanced applications of cryptography and homomorphic encryption, information theory has shaped the foundation of AI, enabling it to reach new heights.</p>
<p>The future is bright as we continue to harness the power of information theory in AI, unlocking the potential for groundbreaking discoveries and innovations that will reshape our world. As we forge ahead, let us celebrate the joy of AI and the transformative impact of information theory on our lives.</p>
<h3 id="6.2-Final-Thoughts:-Let's-Keep-the-AI-Party-Going!">6.2 Final Thoughts: Let's Keep the AI Party Going!<a class="anchor-link" href="#6.2-Final-Thoughts:-Let's-Keep-the-AI-Party-Going!">&para;</a></h3><p>As our mathematical adventure comes to an end, let us remember the optimism, positivity, and humor that have carried us through this journey. The AI party is just getting started, and we have so much more to explore and discover together!</p>
<p>So, let's raise a toast to the dynamic duo of cryptography and AI, the secret sauce of information theory in artificial intelligence, and the ever-elusive black box that sparks our curiosity and fuels our passion for learning. Cheers to the future of AI, and let's keep the party going!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="7.-References">7. References<a class="anchor-link" href="#7.-References">&para;</a></h2><ol>
<li>Shannon, C. E. (1948). <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">A Mathematical Theory of Communication</a>. Bell System Technical Journal, 27(3), 379-423.</li>
<li>Cover, T. M., &amp; Thomas, J. A. (2006). <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">Elements of Information Theory</a>. Wiley-Interscience.</li>
<li>Turing, A. M. (1936). <a href="http://www.abelard.org/turpap2/tp2-ie.asp">On Computable Numbers, with an Application to the Entscheidungsproblem</a>. Proceedings of the London Mathematical Society, 2(1), 230-265.</li>
<li>Vapnik, V., &amp; Chervonenkis, A. (1971). <a href="https://link.springer.com/article/10.1007/BF01079969">On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities</a>. Theory of Probability &amp; Its Applications, 16(2), 264-280.</li>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <a href="https://www.deeplearningbook.org/">Deep Learning</a>. MIT Press.</li>
<li>Lecun, Y., Bengio, Y., &amp; Hinton, G. (2015). <a href="https://www.nature.com/articles/nature14539">Deep learning</a>. Nature, 521(7553), 436-444.</li>
<li>Bialek, W. (2012). <a href="https://press.princeton.edu/books/hardcover/9780691138916/biophysics">Biophysics: Searching for Principles</a>. Princeton University Press.</li>
<li>Tishby, N., Pereira, F. C., &amp; Bialek, W. (1999). <a href="https://arxiv.org/abs/physics/0004057">The information bottleneck method</a>. arXiv preprint physics/0004057.</li>
<li>Gersho, A., &amp; Gray, R. M. (1992). <a href="https://link.springer.com/book/10.1007/978-1-4615-3626-0">Vector Quantization and Signal Compression</a>. Springer Science &amp; Business Media.</li>
<li>Rissanen, J. (1978). <a href="https://www.sciencedirect.com/science/article/pii/0022000078900447">Modeling by shortest data description</a>. Automatica, 14(5), 465-471.</li>
<li>Rivest, R. L., Shamir, A., &amp; Adleman, L. (1978). <a href="https://doi.org/10.1145/359340.359342">A method for obtaining digital signatures and public-key cryptosystems</a>. Communications of the ACM, 21(2), 120-126.</li>
<li>Gentry, C. (2009). <a href="https://doi.org/10.7907/Z9X63JTX">A fully homomorphic encryption scheme</a>. PhD thesis, Stanford University.</li>
<li>Breiman, L., Friedman, J., Stone, C. J., &amp; Olshen, R. A. (1984). <a href="https://www.taylorfrancis.com/books/9781315139470">Classification and Regression Trees</a>. CRC Press.</li>
</ol>
</div>
</div>
</div>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.6/dist/katex.min.css" integrity="sha384-mXD7x5S50Ko38scHSnD4egvoExgMPbrseZorkbE49evAfv9nNcbrXJ8LLNsDgh9d" rel="stylesheet"/>
<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script crossorigin="anonymous" defer="" integrity="sha384-j/ZricySXBnNMJy9meJCtyXTKMhIJ42heyr7oAdxTDBy/CYA9hzpMo+YTNV5C+1X" src="https://cdn.jsdelivr.net/npm/katex@0.16.6/dist/katex.min.js"></script>
<!-- To automatically render math in text elements, include the auto-render extension: -->
<script crossorigin="anonymous" defer="" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" onload="renderMathInElement(document.body);" src="https://cdn.jsdelivr.net/npm/katex@0.16.6/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
                delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
            ],
            throwOnError : false
        });
    });
    </script>

    <!-- <div class="aspect-w-16 aspect-h-9 mx-auto"></div> CSS placeholder -->
  </div>
  <footer class="flex flex-col mt-10 ">
    <ul class="flex flex-wrap">
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/ai-interpretability.html">AI interpretability</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/artificial-intelligence.html">artificial intelligence</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/channel-capacity.html">channel capacity</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/cryptography.html">cryptography</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/entropy.html">entropy</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/error-correcting-codes.html">error-correcting codes</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/feature-importance.html">feature importance</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/information-bottleneck.html">information bottleneck</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/information-theory.html">information theory</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/lossless-compression.html">lossless compression</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/minimum-description-length.html">minimum description length</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/mutual-information.html">mutual information</a>
        </li>
        <li
          class="bg-zinc-200 hover:bg-zinc-300 dark:hover:bg-zinc-800 dark:bg-zinc-700 text-zinc-600 dark:text-zinc-300 mb-2 mr-2 px-3 py-1.5 rounded-md transition ease-in active:-translate-y-1 active:scale-110 duration-75">
          <a href="/tag/rate-distortion-theory.html">rate-distortion theory</a>
        </li>
    </ul>
    <div class="flex w-full my-2 bg-zinc-200 dark:bg-zinc-700 rounded-lg">
      <div class="w-1/2 hover:bg-zinc-300 dark:hover:bg-zinc-800 rounded-l-lg">
        <a class="flex flex-col pr-2" href="/the-intersection-of-ethereum-and-privacy-a-comprehensive-guide-to-zero-knowledge-smart-contracts.html">
          <div class="mx-4 py-2 text-left">
            <p class="text-zinc-600 dark:text-zinc-300 text-sm">« PREV PAGE</p>
            <p class="text-left py-1 hover:underline">The Intersection of Ethereum and Privacy: A Comprehensive Guide to Zero-Knowledge Smart Contracts</p>
          </div>
        </a>
      </div>
      <div class="w-1/2 hover:bg-zinc-300 dark:hover:bg-zinc-800 rounded-r-lg ">
        <a class="flex flex-col" href="/beyond-cryptography-how-steganography-strengthens-blockchains-privacy-armor.html">
          <div class="text-right mx-4 py-2">
            <p class="text-zinc-600 dark:text-zinc-300 text-sm">NEXT PAGE »</p>
            <p class="text-right py-1 hover:underline">Beyond Cryptography: How Steganography Strengthens Blockchain's Privacy Armor</p>
          </div>
        </a>
      </div>
    </div>
  </footer>
  <div>
  </div>
</main>

    </div>
    <footer class="flex w-full text-xs justify-center mt-10 mb-6 text-zinc-600 dark:text-zinc-400">
        <div class="px-4">
            <span>Arcane Analytic &#169; 2023</span>
        </div>
    </footer>


</body>

</html>